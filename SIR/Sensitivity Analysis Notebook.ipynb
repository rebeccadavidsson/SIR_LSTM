{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensitivity Analysis of NPIs used a Hybrid LSTM-SIR model\n",
    "\n",
    "### Here, an analysis will be done to identify the sensitivity of OxCGRT-variables (non-pharmaceutical interventions, NPIs). In particular, sensitivity coefficient (E) will be computed to determine the impact of changes in the dataset and how sensitive model predictions are to missing data or a small input dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import covsirphy as cs\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from core.sensitivityanalysis import SA\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving datasets from COVID-19 Data Hub: https://covid19datahub.io/\n",
      "\n",
      "Please set verbose=2 to see the detailed citation list.\n",
      "\n",
      "\n",
      "Retrieving COVID-19 dataset in Japan from https://github.com/lisphilar/covid19-sir/data/japan\n"
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "parameters = [\"window\", \"steps\", \"delay\", \"clustering threshold\"]\n",
    "\n",
    "# NPI-variables\n",
    "interventions = ['Stringency_index', 'School_closing', 'Workplace_closing', 'Cancel_events',\n",
    " 'Gatherings_restrictions', 'Transport_closing', 'Stay_home_restrictions',\n",
    " 'Internal_movement_restrictions', 'International_movement_restrictions',\n",
    " 'Information_campaigns', 'Testing_policy', 'Contact_tracing']\n",
    "\n",
    "# Select country\n",
    "COUNTRY = \"Italy\"\n",
    "\n",
    "# Download datasets\n",
    "data_loader = cs.DataLoader(\"input\")\n",
    "jhu_data = data_loader.jhu()\n",
    "population_data = data_loader.population()\n",
    "oxcgrt_data = data_loader.oxcgrt()\n",
    "s = cs.Scenario(jhu_data, population_data, country=COUNTRY)\n",
    "days_delay, df_periods = s.estimate_delay(oxcgrt_data)\n",
    "oxcgrt_data = oxcgrt_data.cleaned()\n",
    "jhu_data = jhu_data.cleaned()\n",
    "population_data = population_data.cleaned()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_rates = [0.1, 0.5]\n",
    "PARAMS = {\n",
    "    \"window\": 7,\n",
    "    \"steps\": 5,\n",
    "    \"days_delay\": days_delay,\n",
    "    \"df_periods\": df_periods,\n",
    "    \"TARGET\": interventions[0],\n",
    "    \"input_rate\": input_rates[0]\n",
    "}\n",
    "\n",
    "results_intervention = {}\n",
    "for intervention in interventions:\n",
    "    results = {}\n",
    "    for input_rate in input_rates:\n",
    "        PARAMS[\"TARGET\"] = intervention\n",
    "        PARAMS[\"input_rate\"] = input_rate\n",
    "        print(input_rate, intervention)\n",
    "        try:\n",
    "            errors = SA(COUNTRY, PARAMS, oxcgrt_data, jhu_data, population_data)\n",
    "        except: \n",
    "            print(\"Not enough data for the NPI \", intervention)\n",
    "            continue \n",
    "        results[input_rate] = errors\n",
    "    results_intervention[intervention] = results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = results_intervantion.copy()\n",
    "concat_dfs = []\n",
    "strings1 = list(res.keys())\n",
    "new_strings1 = [string + \"0.1\" for string in strings1]\n",
    "df_1 = pd.DataFrame(columns = new_strings1)\n",
    "for npi in res:\n",
    "    df_1[npi + '0.1'] = res[npi][0.1]\n",
    "\n",
    "strings5 = list(res.keys())\n",
    "new_strings5 = [string + \"0.5\" for string in strings1]\n",
    "df_5 = pd.DataFrame(columns = new_strings5)\n",
    "for npi in res:\n",
    "    df_5[npi + '0.5'] = res[npi][0.5]\n",
    "\n",
    "total = pd.concat([df_1, df_5], axis=1)\n",
    "concat_dfs.append(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = pd.concat(concat_dfs)\n",
    "\n",
    "diff = pd.DataFrame()\n",
    "for col in interventions:\n",
    "    name = col.replace(\"_\", \" \")\n",
    "    if name == \"Stringency index\":\n",
    "        name = \"Stringency index / Lockdown\"\n",
    "\n",
    "    diff[name] = abs(means[col + '0.1'] - means[col + '0.5'])\n",
    "\n",
    "diff = diff.reindex(diff.mean().sort_values().index, axis=1)\n",
    "\n",
    "fig = go.Figure()\n",
    "for col in diff:\n",
    "    fig.add_trace(go.Box(x=diff[col].values, name=diff[col].name, marker_color=\"orange\"))\n",
    "    \n",
    "fig.update_xaxes(title_text=\"<b>Sensitivity CoefficiÃ«nt E</b>\")\n",
    "fig.update_layout(template=\"none\", title=\"NPI Sensitivity analysis of M1\")\n",
    "\n",
    "fig.update_layout(\n",
    "    autosize=True,\n",
    "    width=900,\n",
    "    height=500,\n",
    "    margin=go.layout.Margin(\n",
    "        l=200\n",
    "    )\n",
    "    )\n",
    "fig.layout.font.family = 'Arial'\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Sensitivity Analysis](../SIR/SA_M1.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
