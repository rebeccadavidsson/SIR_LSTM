{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"UQ_toolbox\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import uncertainty_toolbox as uct\n",
    "import pickle\n",
    "import uncertainty_toolbox.viz as uviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (1/n) Calculating accuracy metrics\n",
      " (2/n) Calculating average calibration metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:00<00:01,  6.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3/n) Calculating adversarial group calibration metrics\n",
      "  [1/2] for mean absolute calibration error\n",
      "Measuring adversarial group calibration by spanning group size between 0.0 and 1.0, in 10 intervals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  6.47it/s]\n",
      " 10%|█         | 1/10 [00:00<00:01,  6.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [2/2] for root mean squared calibration error\n",
      "Measuring adversarial group calibration by spanning group size between 0.0 and 1.0, in 10 intervals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  5.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (4/n) Calculating sharpness metrics\n",
      " (n/n) Calculating proper scoring rule metrics\n",
      "**Finished Calculating All Metrics**\n",
      "\n",
      "\n",
      "===================== Accuracy Metrics =====================\n",
      "  MAE           1463.153\n",
      "  RMSE          1747.521\n",
      "  MDAE          1415.351\n",
      "  MARPD         25.398\n",
      "  R2            0.452\n",
      "  Correlation   0.866\n",
      "=============== Average Calibration Metrics ================\n",
      "  Root-mean-squared Calibration Error   0.101\n",
      "  Mean-absolute Calibration Error       0.083\n",
      "  Miscalibration Area                   0.083\n",
      "========== Adversarial Group Calibration Metrics ===========\n",
      "  Mean-absolute Adversarial Group Calibration Error\n",
      "     Group Size: 0.11 -- Calibration Error: 0.214\n",
      "     Group Size: 0.56 -- Calibration Error: 0.118\n",
      "     Group Size: 1.00 -- Calibration Error: 0.083\n",
      "  Root-mean-squared Adversarial Group Calibration Error\n",
      "     Group Size: 0.11 -- Calibration Error: 0.253\n",
      "     Group Size: 0.56 -- Calibration Error: 0.145\n",
      "     Group Size: 1.00 -- Calibration Error: 0.101\n",
      "==================== Sharpness Metrics =====================\n",
      "  Sharpness   3937.167\n",
      "=================== Scoring Rule Metrics ===================\n",
      "  Negative-log-likelihood   22.889\n",
      "  CRPS                      1172.315\n",
      "  Check Score               591.661\n",
      "  Interval Score            6615.433\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# df = pickle.load(open( '../netherlands_UQ.p', \"rb\" ))\n",
    "df = pickle.load(open( '../UQ.p', \"rb\" ))\n",
    "y = df[\"valCases\"].dropna().values[-56:]\n",
    "predictions = df.iloc[:, 2:].mean(axis=1).dropna()[-len(y):].values\n",
    "predictions_std = df.iloc[:, 3:].std(axis=1).dropna()[-len(y):].values \n",
    "bounds = [np.min(predictions) / 3.5, np.max(predictions) * 1.5]\n",
    "x = np.linspace(bounds[0], bounds[1], len(y))\n",
    "\n",
    "# Compute all uncertainty metrics\n",
    "metrics = uct.metrics.get_all_metrics(predictions, predictions_std, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bounds = [np.min(predictions) / 2, np.max(predictions) * 1.5]\n",
    "x = np.linspace(bounds[0], bounds[1], len(y))\n",
    "\n",
    "# Make xy plot\n",
    "uviz.plotly_xy(predictions, predictions_std, y, x, n_subset=int(len(y) / 2.5), ylims=bounds)\n",
    "uviz.plotly_intervals(predictions, predictions_std, y, n_subset=int(len(y) / 2.5), ylims=[1500, 12000])\n",
    "uviz.plotly_calibration(predictions, predictions_std, y)\n",
    "uviz.plotly_intervals_ordered(predictions, predictions_std, y, n_subset=int(len(y) / 1.5), ylims=[0, 20000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Calibration](calibration.png)\n",
    "![Calibration](pred_ordered_interval.png)\n",
    "![Calibration](pred_interval.png)\n",
    "![Calibration](conf_band.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
